{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from recpack.preprocessing.preprocessors import DataFramePreprocessor\n",
    "from recpack.preprocessing.filters import Deduplicate, MinRating, MinItemsPerUser\n",
    "from recpack.scenarios import WeakGeneralization\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "# helpers & metrics\n",
    "from src.helper_functions.data_formatting import *\n",
    "from src.helper_functions.metrics_accuracy import *\n",
    "from src.helper_functions.metrics_coverage import *\n",
    "from src.helper_functions.metrics_exposure import *\n",
    "\n",
    "# models\n",
    "from src.recommenders.ease import myEASE\n",
    "from src.recommenders.slim_bn import BNSLIM\n",
    "from src.recommenders.fslr import FSLR\n",
    "from src.recommenders.slim_bn_admm import BNSLIM_ADMM\n",
    "from src.recommenders.mf_fair import FairMF\n",
    "from src.recommenders.fda import FDA_bpr\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "# import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load ratings.dat from ml-1m folder\n",
    "ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep=\"::\", header=None, usecols=[0,1,2,3], names=[\"User_id\",\"Item_id\",\"Rating\",\"Timestamp\"], engine=\"python\")\n",
    "\n",
    "# load movies.dat from ml-1m folder\n",
    "movies = pd.read_csv(\"ml-1m/movies.dat\", sep=\"::\", header=None, usecols=[0,2], names=[\"Item_id\", \"Genre\"], encoding=\"latin-1\", engine=\"python\")\n",
    "movies[\"Genre\"] = movies[\"Genre\"].apply(lambda x: x.split(\"|\"))\n",
    "movies = movies.explode(\"Genre\")\n",
    "\n",
    "# load users.dat from ml-1m folder\n",
    "users = pd.read_csv(\"ml-1m/users.dat\", sep=\"::\", header=None, usecols=[0,1], names=[\"User_id\", \"Gender\"], encoding=\"latin-1\", engine=\"python\")\n",
    "\n",
    "# replace \"M\" with 0 and \"F\" with 1 in the \"Gender\" column\n",
    "users[\"Gender\"] = users[\"Gender\"].replace({\"M\": 0, \"F\": 1})\n",
    "\n",
    "# join ratings on users with User_id\n",
    "ratings = pd.merge(ratings, users, on=\"User_id\", how=\"left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ratings_pp = DataFramePreprocessor(\"Item_id\", \"User_id\",\"Timestamp\")\n",
    "\n",
    "# define filters\n",
    "deduplicate = Deduplicate(\"Item_id\", \"User_id\", \"Timestamp\")\n",
    "min_rating_filter = MinRating(4, \"Rating\")\n",
    "min_items_per_user_filter = MinItemsPerUser(10, \"Item_id\", \"User_id\")\n",
    "\n",
    "# add filters to pre-processor\n",
    "ratings_pp.add_filter(deduplicate)\n",
    "ratings_pp.add_filter(min_rating_filter)\n",
    "ratings_pp.add_filter(min_items_per_user_filter)\n",
    "\n",
    "# create interaction matrix object\n",
    "im = ratings_pp.process(ratings)\n",
    "\n",
    "# apply filters to ratings frame directly\n",
    "ratings = min_items_per_user_filter.apply(min_rating_filter.apply(deduplicate.apply(ratings)))\n",
    "\n",
    "movies = movies[movies[\"Item_id\"].isin(ratings[\"Item_id\"].unique())] # only keep items that are in the filtered ratings\n",
    "raw_genre_dict = dict(movies.groupby(\"Genre\")[\"Item_id\"].apply(lambda x: list(set(x))))\n",
    "\n",
    "# genre - inner iids dictionary\n",
    "inner_genre_dict = {\n",
    "    genre: get_inner_item_ids(ratings_pp, raw_iids)\n",
    "    for genre, raw_iids in raw_genre_dict.items()\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# compute sparsity after filtering\n",
    "sparsity = 1 - im.density\n",
    "\n",
    "# calculate user interaction and item popularity ranges\n",
    "user_interactions = im.binary_values.sum(axis=1)\n",
    "item_popularities = im.binary_values.sum(axis=0)\n",
    "print(f\"User interaction ranges from {user_interactions.min()} to {user_interactions.max()}. Item popularity ranges from {item_popularities.min()} to {item_popularities.max()}.\")\n",
    "\n",
    "# get the raw ids of all users involved\n",
    "raw_uids = get_raw_user_ids(ratings_pp, im.active_users)\n",
    "\n",
    "# create uid - gender mapping df\n",
    "gender_mapping_df = ratings[ratings[\"User_id\"].isin(raw_uids)][[\"User_id\", \"Gender\"]].drop_duplicates()\n",
    "\n",
    "# get the raw/inner ids of all females involved\n",
    "raw_uids_f = gender_mapping_df.loc[gender_mapping_df[\"Gender\"] == 1, \"User_id\"].to_numpy()\n",
    "inner_uids_f = get_inner_user_ids(ratings_pp, raw_uids_f)\n",
    "\n",
    "# get the raw/inner ids of all males involved\n",
    "raw_uids_m = gender_mapping_df.loc[gender_mapping_df[\"Gender\"] == 0, \"User_id\"].to_numpy()\n",
    "inner_uids_m = get_inner_user_ids(ratings_pp, raw_uids_m)\n",
    "\n",
    "num_interactions_f, num_interactions_m = im.binary_values[inner_uids_f].sum(), im.binary_values[inner_uids_m].sum()\n",
    "\n",
    "# table stats\n",
    "statTable1 = PrettyTable([\"data set\",\"|U|\",\"|I|\",\"int(I)\",\"sparsity\"])\n",
    "statTable1.add_row([\"ML1M\", str(im.num_active_users), str(im.num_active_items), str(im.num_interactions), str(round(sparsity*100,2))])\n",
    "print(statTable1)\n",
    "\n",
    "statTable2 = PrettyTable([\"data set\",\"attribute\",\"|F|\",\"int(F)\",\"|M|\",\"int(M)\"])\n",
    "statTable2.add_row([\"ML1M\", \"gender\", str(len(raw_uids_f)), str(num_interactions_f), str(len(raw_uids_m)), str(num_interactions_m)])\n",
    "print(statTable2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_merged = pd.merge(ratings, movies, on=\"Item_id\", how=\"left\")\n",
    "\n",
    "# Group by gender: 0 for Men and 1 for Women\n",
    "ratings_merged[\"Gender_Group\"] = (ratings_merged[\"Gender\"] == 1).astype(int)\n",
    "\n",
    "# Sum the number of interactions for each user within each genre and gender group\n",
    "user_genre_gender_sum = ratings_merged.groupby([\"User_id\", \"Genre\", \"Gender_Group\"]).size().reset_index(name=\"Interactions\")\n",
    "\n",
    "# Calculate the average number of interactions per user for each genre and gender group\n",
    "genre_gender_avg_interactions = user_genre_gender_sum.groupby([\"Genre\", \"Gender_Group\"])[\"Interactions\"].mean().reset_index()\n",
    "\n",
    "# Pivot the data for plotting\n",
    "genre_gender_avg_interactions_pivot = genre_gender_avg_interactions.pivot(index=\"Genre\", columns=\"Gender_Group\", values=\"Interactions\").fillna(0)\n",
    "\n",
    "# Sort genres by average interactions\n",
    "genre_gender_avg_interactions_pivot[\"Total\"] = (genre_gender_avg_interactions_pivot[0] + genre_gender_avg_interactions_pivot[1]) / 2\n",
    "genre_gender_avg_interactions_pivot_sorted = genre_gender_avg_interactions_pivot.sort_values(by=\"Total\", ascending=False).drop(columns=\"Total\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "genre_gender_avg_interactions_pivot_sorted.plot(kind=\"bar\", figsize=(15,10))\n",
    "\n",
    "plt.title(\"Average Number of Views per User by Genre and Gender\")\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.ylabel(\"Average Number of Views per User\")\n",
    "plt.legend([\"Men\", \"Women\"], title=\"Gender Group\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define K for Top-K\n",
    "K = 10\n",
    "\n",
    "# Define alpha, the parameter that balances the importance of NDCG and Equity in the objective function.\n",
    "# Setting alpha = 0.5 gives equal weight to both metrics, aiming to balance relevance (NDCG) and fairness (Equity).\n",
    "# Adjusting alpha allows for prioritizing one metric over the other.\n",
    "# For instance, setting alpha closer to 1.0 would prioritize NDCG (accuracy), while setting it closer to 0.0 would prioritize Equity (fairness).\n",
    "alpha = 0.2\n",
    "\n",
    "# define seed; seeds tested (1452, 1994, 42, 7, 13800)\n",
    "SEED = 1994\n",
    "\n",
    "# define scenario\n",
    "# Note: Due to the nature of the utilized algorithms (User-User neighborhood methods), \n",
    "# only scenarios that include the 'validation in' set in the 'validation training' set, \n",
    "# and the 'test in' set in the 'full training' set, are applicable.\n",
    "scenario = WeakGeneralization(validation=True, seed=SEED)\n",
    "scenario.split(im)\n",
    "\n",
    "# define time threshold\n",
    "SECONDS = 24*3600\n",
    "\n",
    "# define number of evaluations\n",
    "EVALUATIONS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_objective(model, fit_args={}):\n",
    "    model.fit(scenario.validation_training_data.binary_values, **fit_args)\n",
    "\n",
    "    # generate predictions and mask training interactions\n",
    "    predictions = model.predict(scenario.validation_training_data.binary_values).toarray()\n",
    "    predictions[scenario.validation_training_data.binary_values.nonzero()] = -np.inf\n",
    "\n",
    "    ndcg, _ = tndcg_at_n(predictions, scenario.validation_data_out.binary_values, K)\n",
    "\n",
    "    return 1-ndcg\n",
    "\n",
    "def combined_objective(model, fit_args={}):\n",
    "    model.fit(scenario.validation_training_data.binary_values, **fit_args)\n",
    "\n",
    "    if \"users_features\" in fit_args: #fda\n",
    "        predictions = model.model_.predict().toarray()\n",
    "    else:\n",
    "        predictions = model.predict(scenario.validation_training_data.binary_values).toarray()\n",
    "    predictions[scenario.validation_training_data.binary_values.nonzero()] = -np.inf\n",
    "\n",
    "    ndcg, _ = tndcg_at_n(predictions, scenario.validation_data_out.binary_values, K)\n",
    "    equity, _, _ = c_equity_at_n(predictions[inner_uids_f, :], predictions[inner_uids_m, :], inner_genre_dict, K)\n",
    "\n",
    "    return alpha * (1-ndcg) + (1 - alpha) * equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fairmf\n",
    "sst_field = torch.zeros((im.num_active_users, im.num_active_items), dtype=torch.bool)\n",
    "sst_field[inner_uids_f, :] = True\n",
    "\n",
    "# for fda\n",
    "users_features = np.zeros(im.num_active_users); users_features[inner_uids_m] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize ease\n",
    "optimisation_results_ease = fmin(\n",
    "    fn=lambda param: accuracy_objective(myEASE(l2=param[\"l2\"], method=\"user\")),\n",
    "    space={\"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4))},\n",
    "    algo=tpe.suggest,\n",
    "    timeout = SECONDS,\n",
    "    max_evals = EVALUATIONS,\n",
    ")\n",
    "\n",
    "# optimize bnslim\n",
    "optimisation_results_bnslim = fmin(\n",
    "    fn=lambda param: combined_objective(BNSLIM(knn=100, l1=param[\"l1\"], l2=param[\"l2\"], l3=param[\"l3\"], method=\"user\", seed=SEED), {\"inner_ids_npr\": inner_uids_m}),\n",
    "    space={\"l1\": hp.loguniform(\"l1\", np.log(1e-3), np.log(7)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e-3), np.log(7)),\n",
    "           \"l3\": hp.loguniform(\"l3\", np.log(1e1), np.log(1e4))\n",
    "           }, \n",
    "    algo=tpe.suggest,\n",
    "    timeout=SECONDS,\n",
    "    max_evals=EVALUATIONS\n",
    ")\n",
    "\n",
    "# optimize fslr\n",
    "optimisation_results_fslr = fmin(\n",
    "    fn=lambda param: combined_objective(FSLR(l1=param[\"l1\"], l2=param[\"l2\"], method=\"user\"), {\"inner_ids_pr\": inner_uids_f, \"inner_ids_npr\": inner_uids_m}),\n",
    "    space={\"l1\": hp.loguniform(\"l1\", np.log(1e-3), np.log(1e1)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4))},\n",
    "    algo=tpe.suggest,\n",
    "    timeout=SECONDS,\n",
    "    max_evals=EVALUATIONS\n",
    ")\n",
    "\n",
    "# optimize bnslim admm\n",
    "optimisation_results_bnslim_admm = fmin(\n",
    "    fn=lambda param: combined_objective(BNSLIM_ADMM(l1=param[\"l1\"], l2=param[\"l2\"], l3=param[\"l3\"], method=\"user\"), {\"inner_ids_npr\": inner_uids_m}),\n",
    "    space={\"l1\": hp.loguniform(\"l1\", np.log(1e-3), np.log(50)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4)),\n",
    "           \"l3\": hp.loguniform(\"l3\", np.log(1e-3), np.log(1e3))},\n",
    "    algo=tpe.suggest,\n",
    "    timeout = SECONDS,\n",
    "    max_evals = EVALUATIONS,\n",
    ")\n",
    "\n",
    "# optimize FairMF\n",
    "factor_choices = [32, 64, 128]\n",
    "optimisation_results_fairmf = fmin(\n",
    "    fn=lambda param: combined_objective(FairMF(batch_size=im.num_active_users, learning_rate=param[\"learning_rate\"], l2=param[\"l2\"], num_factors=param[\"num_factors\"], seed=SEED), {\"sst_field\": sst_field}),\n",
    "    space={\"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-6), np.log(1e0)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e-6), np.log(1e-1)),\n",
    "           \"num_factors\": hp.choice(\"num_factors\", factor_choices)\n",
    "           },\n",
    "    algo=tpe.suggest,\n",
    "    timeout=SECONDS,\n",
    "    max_evals=EVALUATIONS\n",
    ")\n",
    "\n",
    "optimisation_results_fairmf[\"num_factors\"] = factor_choices[optimisation_results_fairmf[\"num_factors\"]]\n",
    "\n",
    "# optimize FDA\n",
    "# define the parameter choices\n",
    "num_ng_choices = [5, 7, 9, 10]\n",
    "ratio_choices = [0.1, 0.3, 0.5, 0.7]\n",
    "all_combinations = itertools.product(num_ng_choices, ratio_choices)\n",
    "\n",
    "best_params = None\n",
    "best_score = float(\"inf\")\n",
    "\n",
    "for num_ng, noise_ratio in all_combinations:\n",
    "    score = combined_objective(\n",
    "        FDA_bpr(num_ng=num_ng, noise_ratio=noise_ratio, seed=SEED), \n",
    "        {\"users_features\": users_features}\n",
    "    )\n",
    "\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_params = {\"num_ng\": num_ng, \"noise_ratio\": noise_ratio}\n",
    "\n",
    "opt_params = {}\n",
    "opt_params.update({\n",
    "    \"ease\": optimisation_results_ease,\n",
    "    \"bnslim\": optimisation_results_bnslim,\n",
    "    \"fslr\": optimisation_results_fslr,\n",
    "    \"bnslim_admm\": optimisation_results_bnslim_admm,\n",
    "    \"fairmf\": optimisation_results_fairmf,\n",
    "    \"fda\": best_params\n",
    "})\n",
    "\n",
    "folder = f\"ml-1m/{SEED}\"; os.makedirs(folder, exist_ok=True)\n",
    "with open(folder + \"/opt_params.json\", \"w\") as f: json.dump(opt_params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"ml-1m/{SEED}/opt_params.json\", \"r\") as f: opt_params = json.load(f)\n",
    "\n",
    "def initialize_models(opt_params):\n",
    "    return {\n",
    "        \"ease\": myEASE(l2=opt_params[\"ease\"][\"l2\"], method=\"user\"),\n",
    "        \"bnslim\": BNSLIM(knn=100, l1=opt_params[\"bnslim\"][\"l1\"], l2=opt_params[\"bnslim\"][\"l2\"], l3=opt_params[\"bnslim\"][\"l3\"], maxIter=50, method=\"user\", seed=SEED),\n",
    "        \"fslr\": FSLR(l1=opt_params[\"fslr\"][\"l1\"], l2=opt_params[\"fslr\"][\"l2\"], method=\"user\"),\n",
    "        \"bnslim_admm\": BNSLIM_ADMM(l1=opt_params[\"bnslim_admm\"][\"l1\"], l2=opt_params[\"bnslim_admm\"][\"l2\"], l3=opt_params[\"bnslim_admm\"][\"l3\"], method=\"user\"),\n",
    "        \"fairmf\": FairMF(batch_size=im.num_active_users, l2=opt_params[\"fairmf\"][\"l2\"], learning_rate=opt_params[\"fairmf\"][\"learning_rate\"], num_factors=opt_params[\"fairmf\"][\"num_factors\"], seed=SEED),\n",
    "        \"fda\": FDA_bpr(\n",
    "            noise_ratio=opt_params[\"fda\"][\"noise_ratio\"], \n",
    "            num_ng=opt_params[\"fda\"][\"num_ng\"],\n",
    "            seed=SEED\n",
    "        )\n",
    "    }\n",
    "\n",
    "# initialize models\n",
    "models = initialize_models(opt_params)\n",
    "\n",
    "# define the models, list sizes, and metrics\n",
    "list_sizes = [10, 20, 50, 100]\n",
    "metrics = [\"ndcg\", \"recall\", \"c-equity\", \"u-parity\"]\n",
    "\n",
    "# initialize a dictionary to store results with mean and standard deviation\n",
    "results = {\n",
    "    \"iters_num\": {model: 0 for model in [\"bnslim\", \"fslr\", \"bnslim_admm\", \"fairmf\"]},\n",
    "    \"fit_time\": {model: 0 for model in models.keys()},\n",
    "    **{metric: {model: {size: {\"mean\": 0, \"std\": 0} for size in list_sizes} for model in models.keys()} for metric in metrics},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    \n",
    "    print(f\"Training model {model_name}...\")\n",
    "\n",
    "    params = {}\n",
    "    if model_name == \"fslr\":\n",
    "        params = {\"inner_ids_pr\": inner_uids_f, \"inner_ids_npr\": inner_uids_m}\n",
    "    elif model_name in [\"bnslim\", \"bnslim_admm\"]:\n",
    "        params = {\"inner_ids_npr\": inner_uids_m}\n",
    "    elif model_name == \"fairmf\":\n",
    "        params = {\"sst_field\": sst_field}\n",
    "    elif model_name == \"fda\":\n",
    "        params = {\"users_features\": users_features}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(scenario.full_training_data.binary_values, **params)\n",
    "    results[\"fit_time\"][model_name] = time.time() - start_time\n",
    "\n",
    "    if model_name in results[\"iters_num\"]:\n",
    "        if model_name == \"fairmf\":\n",
    "            results[\"iters_num\"][model_name] = model.epochs\n",
    "        else:\n",
    "            results[\"iters_num\"][model_name] = model.iters\n",
    "\n",
    "    # generate predictions and mask training interactions\n",
    "    if model_name == \"fda\":\n",
    "        y_pred = model.model_.predict()\n",
    "    else:\n",
    "        y_pred = model.predict(scenario.full_training_data.binary_values)\n",
    "    predictions = y_pred.toarray()\n",
    "    predictions[scenario.full_training_data.binary_values.nonzero()] = -np.inf\n",
    "\n",
    "    # compute evaluation metrics for different values of K\n",
    "    for K in list_sizes:\n",
    "        # accuracy metrics\n",
    "        results[\"ndcg\"][model_name][K][\"mean\"], results[\"ndcg\"][model_name][K][\"std\"] = tndcg_at_n(predictions, scenario.test_data_out.binary_values, K)\n",
    "        results[\"recall\"][model_name][K][\"mean\"], results[\"recall\"][model_name][K][\"std\"] = recall_at_n(predictions, scenario.test_data_out.binary_values, K)\n",
    "\n",
    "        # fairness metrics\n",
    "        results[\"c-equity\"][model_name][K][\"mean\"], results[\"c-equity\"][model_name][K][\"std\"], _ = c_equity_at_n(predictions[inner_uids_f, :], predictions[inner_uids_m, :], inner_genre_dict, K)\n",
    "\n",
    "        females = np.ones(im.num_active_users); females[inner_uids_m] = 0\n",
    "        results[\"u-parity\"][model_name][K][\"mean\"], results[\"u-parity\"][model_name][K][\"std\"] = u_parity_at_n(predictions, females, inner_genre_dict, K)\n",
    "\n",
    "    # # save model\n",
    "    # pickle.dump(model, open(f\"ml-1m/{SEED}/{model_name}.pkl\", \"wb\"))\n",
    "\n",
    "# save results\n",
    "with open(\"ml-1m/{SEED}/results.json\", \"w\") as f: json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
