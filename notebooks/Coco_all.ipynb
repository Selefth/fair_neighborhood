{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from recpack.preprocessing.preprocessors import DataFramePreprocessor\n",
    "from recpack.preprocessing.filters import Deduplicate, MinRating, MinItemsPerUser\n",
    "from recpack.scenarios import StrongGeneralization\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "# helpers & metrics\n",
    "from src.helper_functions.data_formatting import *\n",
    "from src.helper_functions.metrics_accuracy import *\n",
    "from src.helper_functions.metrics_coverage import *\n",
    "from src.helper_functions.metrics_exposure import *\n",
    "\n",
    "# models\n",
    "from src.recommenders.ease import myEASE\n",
    "from src.recommenders.slim_bn import BNSLIM\n",
    "from src.recommenders.fslr import FSLR\n",
    "from src.recommenders.slim_bn_admm import BNSLIM_ADMM\n",
    "from src.recommenders.mf_fair import FairMF\n",
    "\n",
    "import os, json\n",
    "import time\n",
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"coco/COCO.csv\")\n",
    "\n",
    "# Selecting the required columns and rows with a single country\n",
    "ratings = ratings[ratings['country'].str.count('\\|') == 0][['user', 'item', 'rating', 'country']]\n",
    "\n",
    "# https://www.gov.uk/government/publications/countries-defined-as-developing-by-the-oecd/countries-defined-as-developing-by-the-oecd\n",
    "\n",
    "developing_countries = [\n",
    "    \"Algeria\", \"Argentina\", \"Armenia\", \"Bangladesh\", \"Belize\", \"Bolivia\",\n",
    "    \"Brazil\", \"China\", \"Côte D’Ivoire\", \"Egypt\", \"Ethiopia\", \"Fiji\",\n",
    "    \"Guatemala\", \"India\", \"Indonesia\", \"Iran\", \"Jordan\", \"Lebanon\",\n",
    "    \"Malawi\", \"Malaysia\", \"Mexico\", \"Morocco\", \"Mozambique\", \"Nepal\",\n",
    "    \"Pakistan\", \"Papua New Guinea\", \"Philippines\", \"Serbia\", \"South Africa\",\n",
    "    \"Sri Lanka\", \"Surinam\", \"Syria\", \"Thailand\", \"Timor-Leste\", \"Tonga\",\n",
    "    \"Turkey\", \"Ukraine\", \"Uzbekistan\", \"Venezuela\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "ratings['developing'] = ratings['country'].apply(lambda x: 1 if x in developing_countries else 0)\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_pp = DataFramePreprocessor(\"item\", \"user\")\n",
    "\n",
    "# define filters\n",
    "deduplicate = Deduplicate(\"item\", \"user\")\n",
    "min_rating_filter = MinRating(5, \"rating\")\n",
    "min_items_per_user_filter = MinItemsPerUser(10, \"item\", \"user\")\n",
    "\n",
    "# add filters to pre-processor\n",
    "ratings_pp.add_filter(deduplicate)\n",
    "ratings_pp.add_filter(min_rating_filter)\n",
    "ratings_pp.add_filter(min_items_per_user_filter)\n",
    "\n",
    "# create interaction matrix object\n",
    "im = ratings_pp.process(ratings)\n",
    "\n",
    "# apply filters to ratings frame directly\n",
    "ratings = min_items_per_user_filter.apply(min_rating_filter.apply(deduplicate.apply(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sparsity after filtering\n",
    "sparsity = 1 - im.density\n",
    "\n",
    "# calculate user interaction and item popularity ranges\n",
    "user_interactions = im.binary_values.sum(axis=1)\n",
    "item_popularities = im.binary_values.sum(axis=0)\n",
    "print(f\"User interaction ranges from {user_interactions.min()} to {user_interactions.max()}. Item popularity ranges from {item_popularities.min()} to {item_popularities.max()}.\")\n",
    "\n",
    "# get the raw ids of all items involved\n",
    "raw_iids = get_raw_item_ids(ratings_pp, im.active_items)\n",
    "\n",
    "# create iid - gender mapping df\n",
    "gender_papping_df = ratings[ratings[\"item\"].isin(raw_iids)][[\"item\", \"developing\"]].drop_duplicates()\n",
    "\n",
    "# get the raw/inner ids of all females involved\n",
    "raw_iids_np = gender_papping_df.loc[gender_papping_df[\"developing\"] == 0, \"item\"].to_numpy()\n",
    "inner_iids_np = get_inner_item_ids(ratings_pp, raw_iids_np)\n",
    "\n",
    "# get the raw/inner ids of all males involved\n",
    "raw_iids_p = gender_papping_df.loc[gender_papping_df[\"developing\"] == 1, \"item\"].to_numpy()\n",
    "inner_iids_p = get_inner_item_ids(ratings_pp, raw_iids_p)\n",
    "\n",
    "items_dict = {\n",
    "    \"protected\": inner_iids_p,\n",
    "    \"non-protected\": inner_iids_np\n",
    "}\n",
    "\n",
    "num_interactions_np, num_interactions_p = im.binary_values[:, inner_iids_np].sum(), im.binary_values[:, inner_iids_p].sum()\n",
    "\n",
    "# table stats\n",
    "statTable1 = PrettyTable([\"data set\",\"|U|\",\"|I|\",\"int(I)\",\"sparsity\"])\n",
    "statTable1.add_row([\"COCO\", str(im.num_active_users), str(im.num_active_items), str(im.num_interactions), str(round(sparsity*100,2))])\n",
    "print(statTable1)\n",
    "\n",
    "statTable2 = PrettyTable([\"data set\",\"attribute\",\"|developing|\",\"int(developing)\",\"|developed|\",\"int(developed)\"])\n",
    "statTable2.add_row([\"COCO\", \"country\", str(len(raw_iids_p)), str(num_interactions_p), str(len(raw_iids_np)), str(num_interactions_np)])\n",
    "print(statTable2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define K for Top-K\n",
    "K = 10\n",
    "\n",
    "# Define alpha, the parameter that balances the importance of NDCG and BDV in the objective function.\n",
    "# Setting alpha = 0.5 gives equal weight to both metrics, aiming to balance relevance (NDCG) and fairness (BDV).\n",
    "# Adjusting alpha allows for prioritizing one metric over the other.\n",
    "# For instance, setting alpha closer to 1.0 would prioritize NDCG (accuracy), while setting it closer to 0.0 would prioritize BDV (fairness).\n",
    "alpha = 0.2\n",
    "\n",
    "# define seed; seeds tested (1452, 1994, 42, 7, 13800)\n",
    "SEED = 7\n",
    "\n",
    "# define scenario\n",
    "scenario = StrongGeneralization(validation=True, seed=SEED)\n",
    "scenario.split(im)\n",
    "\n",
    "# define time threshold\n",
    "SECONDS = 24*3600\n",
    "\n",
    "# define number of evaluations\n",
    "EVALUATIONS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_objective(model, fit_args={}):\n",
    "    model.fit(scenario.validation_training_data.binary_values, **fit_args)\n",
    "\n",
    "    valid_rows = scenario.validation_data_in.binary_values.sum(axis=1).A1 != 0\n",
    "    filtered_validation_data_in = scenario.validation_data_in.binary_values[valid_rows]\n",
    "    filtered_validation_data_out = scenario.validation_data_out.binary_values[valid_rows]\n",
    "\n",
    "    # generate predictions and mask training interactions\n",
    "    predictions = model.predict(filtered_validation_data_in).toarray()\n",
    "    predictions[filtered_validation_data_in.nonzero()] = -np.inf\n",
    "\n",
    "    ndcg, _ = tndcg_at_n(predictions, filtered_validation_data_out, K)\n",
    "\n",
    "    return 1-ndcg\n",
    "\n",
    "def combined_objective(model, fit_args={}):\n",
    "    model.fit(scenario.validation_training_data.binary_values, **fit_args)\n",
    "\n",
    "    valid_rows = scenario.validation_data_in.binary_values.sum(axis=1).A1 != 0\n",
    "    filtered_validation_data_in = scenario.validation_data_in.binary_values[valid_rows]\n",
    "    filtered_validation_data_out = scenario.validation_data_out.binary_values[valid_rows]\n",
    "\n",
    "    predictions = model.predict(filtered_validation_data_in).toarray()\n",
    "    predictions[filtered_validation_data_in.nonzero()] = -np.inf\n",
    "\n",
    "    ndcg, _ = tndcg_at_n(predictions, filtered_validation_data_out, K)\n",
    "    bdv = bdv_at_n(predictions, items_dict, K)\n",
    "\n",
    "    return alpha * (1-ndcg) + (1 - alpha) * bdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for FairMF\n",
    "sst_field = torch.zeros((im.num_active_users, im.num_active_items), dtype=torch.bool)\n",
    "sst_field[:, inner_iids_p] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize ease\n",
    "optimisation_results_ease = fmin(\n",
    "    fn=lambda param: accuracy_objective(myEASE(l2=param[\"l2\"])),\n",
    "    space={\"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4))},\n",
    "    algo=tpe.suggest,\n",
    "    timeout = SECONDS,\n",
    "    max_evals = EVALUATIONS,\n",
    ")\n",
    "\n",
    "# optimize fslr\n",
    "optimisation_results_fslr = fmin(\n",
    "    fn=lambda param: combined_objective(FSLR(l1=param[\"l1\"], l2=param[\"l2\"]), {\"inner_ids_pr\": inner_iids_p, \"inner_ids_npr\": inner_iids_np}),\n",
    "    space={\"l1\": hp.loguniform(\"l1\", np.log(1e-3), np.log(1e1)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4))},\n",
    "    algo=tpe.suggest,\n",
    "    timeout=SECONDS,\n",
    "    max_evals=EVALUATIONS\n",
    ")\n",
    "\n",
    "# optimize bnslim\n",
    "optimisation_results_bnslim = fmin(\n",
    "    fn=lambda param: combined_objective(BNSLIM(knn=100, l1=param[\"l1\"], l2=param[\"l2\"], l3=param[\"l3\"], seed=SEED), {\"inner_ids_npr\": inner_iids_np}),\n",
    "    space={\"l1\": hp.loguniform(\"l1\", np.log(1e-3), np.log(7)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e-3), np.log(7)),\n",
    "           \"l3\": hp.loguniform(\"l3\", np.log(1e1), np.log(1e4))\n",
    "           }, \n",
    "    algo=tpe.suggest,\n",
    "    timeout=SECONDS,\n",
    "    max_evals=EVALUATIONS\n",
    ")\n",
    "\n",
    "# optimize bnslim admm\n",
    "optimisation_results_bnslim_admm = fmin(\n",
    "    fn=lambda param: combined_objective(BNSLIM_ADMM(l1=param[\"l1\"], l2=param[\"l2\"], l3=param[\"l3\"]), {\"inner_ids_npr\": inner_iids_np}),\n",
    "    space={\"l1\": hp.loguniform(\"l1\", np.log(1e-3), np.log(50)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4)),\n",
    "           \"l3\": hp.loguniform(\"l3\", np.log(1e-3), np.log(1e3))},\n",
    "    algo=tpe.suggest,\n",
    "    timeout = SECONDS,\n",
    "    max_evals = EVALUATIONS,\n",
    ")\n",
    "\n",
    "# optimize FairMF\n",
    "factor_choices = [32, 64, 128]\n",
    "optimisation_results_fairmf = fmin(\n",
    "    fn=lambda param: combined_objective(FairMF(batch_size=im.num_active_users, learning_rate=param[\"learning_rate\"], l2=param[\"l2\"], num_factors=param[\"num_factors\"], seed=SEED), {\"sst_field\": sst_field}),\n",
    "    space={\"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-6), np.log(1e0)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e-6), np.log(1e-1)),\n",
    "           \"num_factors\": hp.choice(\"num_factors\", factor_choices)\n",
    "           },\n",
    "    algo=tpe.suggest,\n",
    "    timeout=SECONDS,\n",
    "    max_evals=EVALUATIONS\n",
    ")\n",
    "\n",
    "optimisation_results_fairmf[\"num_factors\"] = factor_choices[optimisation_results_fairmf[\"num_factors\"]]\n",
    "\n",
    "opt_params = {}\n",
    "opt_params.update({\n",
    "    \"ease\": optimisation_results_ease,\n",
    "    \"fslr\": optimisation_results_fslr,\n",
    "    \"bnslim\": optimisation_results_bnslim,\n",
    "    \"bnslim_admm\": optimisation_results_bnslim_admm,\n",
    "    \"fairmf\": optimisation_results_fairmf\n",
    "})\n",
    "\n",
    "folder = f\"coco/all/{SEED}\"; os.makedirs(folder, exist_ok=True)\n",
    "with open(f\"{folder}/opt_params.json\", \"w\") as f: json.dump(opt_params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"coco/all/{SEED}/opt_params.json\", \"r\") as f: opt_params = json.load(f)\n",
    "\n",
    "def initialize_models(opt_params):\n",
    "    return {\n",
    "        \"ease\": myEASE(l2=opt_params[\"ease\"][\"l2\"]),\n",
    "        \"bnslim\": BNSLIM(knn=100, l1=opt_params[\"bnslim\"][\"l1\"], l2=opt_params[\"bnslim\"][\"l2\"], l3=opt_params[\"bnslim\"][\"l3\"], maxIter=50, seed=SEED),\n",
    "        \"fslr\": FSLR(l1=opt_params[\"fslr\"][\"l1\"], l2=opt_params[\"fslr\"][\"l2\"]),\n",
    "        \"bnslim_admm\": BNSLIM_ADMM(l1=opt_params[\"bnslim_admm\"][\"l1\"], l2=opt_params[\"bnslim_admm\"][\"l2\"], l3=opt_params[\"bnslim_admm\"][\"l3\"]),\n",
    "        \"fairmf\": FairMF(batch_size=im.num_active_users, l2=opt_params[\"fairmf\"][\"l2\"], learning_rate=opt_params[\"fairmf\"][\"learning_rate\"], num_factors=opt_params[\"fairmf\"][\"num_factors\"], seed=SEED),\n",
    "    }\n",
    "\n",
    "# initialize models\n",
    "models = initialize_models(opt_params)\n",
    "\n",
    "# define list sizes and metrics\n",
    "list_sizes = [10, 20, 50, 100]\n",
    "metrics = [\"ndcg\", \"recall\", \"bdv\", \"apcr\"]\n",
    "\n",
    "# initialize results dictionary\n",
    "results = {\n",
    "    \"iters_num\": {model: 0 for model in [\"bnslim\", \"fslr\", \"bnslim_admm\", \"fairmf\"]},\n",
    "    \"fit_time\": {model: 0 for model in models.keys()},\n",
    "    **{metric: {model: {size: {\"mean\": 0, \"std\": 0} if metric in [\"ndcg\", \"recall\"] else 0 for size in list_sizes} for model in models.keys()} for metric in metrics}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    \n",
    "    print(f\"Training model {model_name}...\")\n",
    "\n",
    "    params = {}\n",
    "    if model_name == \"fslr\":\n",
    "        params = {\"inner_ids_pr\": inner_iids_p, \"inner_ids_npr\": inner_iids_np}\n",
    "    elif model_name in [\"bnslim\", \"bnslim_admm\"]:\n",
    "        params = {\"inner_ids_npr\": inner_iids_np}\n",
    "    elif model_name == \"fairmf\":\n",
    "        params = {\"sst_field\": sst_field}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(scenario.full_training_data.binary_values, **params)\n",
    "    results[\"fit_time\"][model_name] = time.time() - start_time\n",
    "\n",
    "    if model_name in results[\"iters_num\"]:\n",
    "        if model_name == \"fairmf\":\n",
    "            results[\"iters_num\"][model_name] = model.epochs\n",
    "        else:\n",
    "            results[\"iters_num\"][model_name] = model.iters\n",
    "\n",
    "    valid_rows = scenario.test_data_in.binary_values.sum(axis=1).A1 != 0 # delete zero rows (strong gen)\n",
    "    filtered_test_data_in = scenario.test_data_in.binary_values[valid_rows]\n",
    "    filtered_test_data_out = scenario.test_data_out.binary_values[valid_rows]\n",
    "\n",
    "    # generate predictions and mask training interactions\n",
    "    predictions = model.predict(filtered_test_data_in).toarray()\n",
    "    predictions[filtered_test_data_in.nonzero()] = -np.inf\n",
    "\n",
    "    # compute evaluation metrics for different values of K\n",
    "    for K in list_sizes:\n",
    "        # accuracy metrics\n",
    "        results[\"ndcg\"][model_name][K][\"mean\"], results[\"ndcg\"][model_name][K][\"std\"] = tndcg_at_n(predictions, filtered_test_data_out, K)\n",
    "        results[\"recall\"][model_name][K][\"mean\"], results[\"recall\"][model_name][K][\"std\"] = recall_at_n(predictions, filtered_test_data_out, K)\n",
    "\n",
    "        # fairness metrics\n",
    "        results[\"bdv\"][model_name][K] = bdv_at_n(predictions, items_dict, K)\n",
    "        results[\"apcr\"][model_name][K] = apcr_at_n(predictions, items_dict, K)\n",
    "\n",
    "    # # save model\n",
    "    # pickle.dump(model, open(f\"coco/all/{SEED}/{model_name}.pkl\", \"wb\"))\n",
    "\n",
    "# save results\n",
    "with open(f\"coco/all/{SEED}/results.json\", \"w\") as f: json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
