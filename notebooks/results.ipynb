{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for summarizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    METHODS = [\"ease\", \"fslr\", \"bnslim\", \"bnslim_admm\", \"fairmf\", \"fda\"]\n",
    "    K_VALUES = [\"10\"]\n",
    "    ACCURACY_METRICS = [\"ndcg\", \"recall\"]\n",
    "    FAIRNESS_METRICS = {\n",
    "        'consumer': [\"c-equity\", \"u-parity\"],\n",
    "        'provider': [\"bdv\", \"apcr\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_value(k: str) -> bool:\n",
    "    return k in Config.K_VALUES\n",
    "\n",
    "def get_rec_methods(method: str) -> bool:\n",
    "    return method in Config.METHODS\n",
    "\n",
    "def format_value(value: float) -> str:\n",
    "    return f\"{value:.3f}\"\n",
    "\n",
    "def get_metrics_for_experiment_type(experiment_type: str):\n",
    "    if experiment_type not in Config.FAIRNESS_METRICS:\n",
    "        raise ValueError(\"Invalid experiment type. Choose 'provider' or 'consumer'.\")\n",
    "    return Config.ACCURACY_METRICS + Config.FAIRNESS_METRICS[experiment_type]\n",
    "\n",
    "def report_metrics(metrics, data):\n",
    "    metric_data = []\n",
    "    for metric in metrics:\n",
    "        for method, values in data[metric].items():\n",
    "            if not get_rec_methods(method):\n",
    "                continue\n",
    "            for k, v in values.items():\n",
    "                if not get_k_value(k):\n",
    "                    continue\n",
    "                if isinstance(v, float):\n",
    "                    score = format_value(v)\n",
    "                else:\n",
    "                    score = f\"{format_value(v['mean'])} ({format_value(v['std'])})\"\n",
    "                metric_data.append({'Metric': metric, 'Method': method, 'K': k, 'Score': score})\n",
    "    return metric_data\n",
    "\n",
    "def compute_trades(data, experiment_type, ALPHA=0.2):\n",
    "    if experiment_type not in [\"consumer\", \"provider\"]:\n",
    "        raise ValueError(\"Invalid experiment type. Choose 'provider' or 'consumer'.\")\n",
    "\n",
    "    trade_off_scores = {}\n",
    "\n",
    "    # select the appropriate fairness metric based on the experiment type\n",
    "    fairness_metric = \"c-equity\" if experiment_type == \"consumer\" else \"bdv\"\n",
    "\n",
    "    for method, ndcg_values in data.get(\"ndcg\", {}).items():\n",
    "        if not get_rec_methods(method):\n",
    "            continue\n",
    "\n",
    "        for k, ndcg_v in ndcg_values.items():\n",
    "            if not get_k_value(k):\n",
    "                continue\n",
    "\n",
    "            ndcg_mean = ndcg_v.get('mean') if isinstance(ndcg_v, dict) else ndcg_v\n",
    "            if ndcg_mean is None:\n",
    "                logging.warning(f\"Missing 'mean' in ndcg for method {method} and k {k}\")\n",
    "                continue\n",
    "\n",
    "            fairness_v = None\n",
    "            if fairness_metric in data and k in data[fairness_metric].get(method, {}):\n",
    "                fairness_val = data[fairness_metric][method][k]\n",
    "                fairness_v = fairness_val.get('mean') if isinstance(fairness_val, dict) else fairness_val\n",
    "            \n",
    "            if fairness_v is not None:\n",
    "                trade_off_value = ALPHA * (1 - ndcg_mean) + (1 - ALPHA) * fairness_v\n",
    "                trade_off_scores.setdefault(method, {}).setdefault(k, []).append(trade_off_value)\n",
    "            else:\n",
    "                logging.warning(f\"Fairness value not found for method {method} and k {k}\")\n",
    "\n",
    "    return trade_off_scores\n",
    "\n",
    "def aggregate_trade_off_scores(trade_off_all_scores):\n",
    "    aggregated_trade_off = {}\n",
    "    for method, k_values in trade_off_all_scores.items():\n",
    "        aggregated_trade_off[method] = {k: {'mean': np.mean(scores), 'std': np.std(scores)}\n",
    "                                        for k, scores in k_values.items()}\n",
    "    return aggregated_trade_off\n",
    "\n",
    "def aggregate_metrics(metrics, data_list):\n",
    "    aggregated_data = {}\n",
    "    for metric in metrics:\n",
    "        aggregated_data[metric] = {}\n",
    "        for method in data_list[0][metric]:\n",
    "            if not get_rec_methods(method):\n",
    "                continue\n",
    "            aggregated_data[metric][method] = {}\n",
    "            for k in data_list[0][metric][method]:\n",
    "                if not get_k_value(k):\n",
    "                    continue\n",
    "                if isinstance(data_list[0][metric][method][k], float):\n",
    "                    means = [data[metric][method][k] for data in data_list]\n",
    "                else:\n",
    "                    means = [data[metric][method][k]['mean'] for data in data_list]\n",
    "                aggregated_data[metric][method][k] = {'mean': np.mean(means), 'std': np.std(means)}\n",
    "    return aggregated_data\n",
    "\n",
    "def aggregate_fit_times(data_list):\n",
    "    aggregated_times = {}\n",
    "    for data in data_list:\n",
    "        for method, time in data[\"fit_time\"].items():\n",
    "            if method not in aggregated_times:\n",
    "                aggregated_times[method] = []\n",
    "            aggregated_times[method].append(time)\n",
    "    return {method: {'mean': np.mean(times), 'std': np.std(times)} for method, times in aggregated_times.items()}\n",
    "\n",
    "def process_data(folder_path: str, experiment_type: str):\n",
    "    folder_path = Path(folder_path)\n",
    "    data_list = []\n",
    "\n",
    "    for folder in folder_path.iterdir():\n",
    "        if folder.is_dir():\n",
    "            seed_path = folder / \"results.json\"\n",
    "            try:\n",
    "                with open(seed_path, \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "                    data_list.append(data)\n",
    "            except (IOError, json.JSONDecodeError) as e:\n",
    "                logging.error(f\"Error processing file {seed_path}: {e}\")\n",
    "\n",
    "    if not data_list:\n",
    "        return {}\n",
    "\n",
    "    # separate accuracy and fairness metrics\n",
    "    accuracy_metrics = Config.ACCURACY_METRICS\n",
    "    fairness_metrics = Config.FAIRNESS_METRICS[experiment_type]\n",
    "\n",
    "    aggregated_data = aggregate_metrics(accuracy_metrics + fairness_metrics, data_list)\n",
    "    aggregated_fit_times = aggregate_fit_times(data_list)\n",
    "\n",
    "    metrics_dfs = {\n",
    "        'accuracy': pd.DataFrame(report_metrics(accuracy_metrics, aggregated_data)),\n",
    "        'fairness': pd.DataFrame(report_metrics(fairness_metrics, aggregated_data)),\n",
    "        'fit_times': pd.DataFrame([{\n",
    "            'Method': method, \n",
    "            'Average Fit Time': f\"{format_value(times['mean'])} ({format_value(times['std'])})\"\n",
    "        } for method, times in aggregated_fit_times.items()])\n",
    "    }\n",
    "\n",
    "    return metrics_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# data = process_data(\"ml-1m\", \"consumer\")\n",
    "data = process_data(\"lastfm-1k\", \"consumer\")\n",
    "# data = process_data(\"coco/all\", \"provider\")\n",
    "# data = process_data(\"coco/subset\", \"provider\")\n",
    "# data = process_data(\"goodreads\", \"provider\")\n",
    "\n",
    "for metric in [\"accuracy\", \"fairness\", \"fit_times\"]:\n",
    "    print(f\"\\n{metric.replace('_', ' ').capitalize()}:\\n\", data.get(metric, pd.DataFrame()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
